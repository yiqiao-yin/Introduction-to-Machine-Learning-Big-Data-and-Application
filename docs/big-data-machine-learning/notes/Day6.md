# Representation Learning

There are three major types of *Representation Learning*.

## Artificial Neural Network

The architecture of a basic Artificial Neural Network (ANN) is the following
$$
\text{input variables:} \rightarrow
[\vdots] \rightarrow [\vdots]
\rightarrow
\text{output: predictions}
$$

When the notion of neural network is mentioned, we assume the architecture above with any particular sets of parameters (number of layers, number of neurons per layer, ..., i.e. these are all tuning parameters). ANN uses two concepts: *forward propagation* and *backward propagation*. 

- *Forward Propagation*: This operation allows information to pass from the input layer all the way to the output layer. There are linear components as well as non-linear components. Recall the basic linear regression and logistic regression models. We have explanatory variables $X$ and we first construct a linear combination of $\sum_{j=1}^p w_j X_j$ while the running index $j$ indicates the $j$th variable. This is then fed into the activation function (the famous ones are ReLU and sigmoid) to create predictions $\hat{Y}$. 
- *Backward Propagation*: This operation allows us to compare the predictions made by the educated guesses generated from the model with the ground truth, i.e. we compare how far away predictions $\hat{Y}$ is from the truth $Y$. This allows us to figure out the exact loss of a model. This is considered as an optimization problem with the following objective function:
$$\min_w \mathcal{L}(\hat{Y}, Y)$$
and the loss function is a matter of choice by the scientist or user of the algorithm. A list of loss functions can be found [here](https://towardsdatascience.com/most-common-loss-functions-in-machine-learning-c7212a99dae0). This above objective function states the following. The goal is to minimize the loss generated by prediction $\hat{Y}$ and the ground truth $Y$ while subject to the constraint of parameters (or weights) of the model $w$. In other words, we are allowed to change our weights $w$ until we found ourselves satisfied with the error generated with $\hat{Y}$ and $Y$. The quantity of error margin is a matter of choice and it usually is carried out as a relative term from model to model (i.e. this means we need a benchmark to compare and we do not just rely on one model).
- *Regressor*: This type of ANN learns from $X$ and produces an estimated value of $Y$ while $Y$ is continuous. The common loss function is [MSE](https://towardsdatascience.com/https-medium-com-chayankathuria-regression-why-mean-square-error-a8cad2a1c96f). In a neural network architecture that is designed as a regressor (predict a continuous variable, i.e. like regression model), we output one neuron.
$$
\text{input variables:} \rightarrow
[\vdots] \rightarrow [\vdots]
\rightarrow
[.], 
\text{output: predictions}
$$

- *Classifier*: This type of ANN learns from $X$ and produes an estimated probability of $Y$ that is a particular discrete value (aka factor or class). The common loss function is [binary cross-entropy](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a). For a neural network architecture that is designed as a classifier (predict a discrete variable, a class, or a label, i.e. like logistic model), we output a certain number of neurons (number should match the number of levels in the response variable). 
$$
\text{input variables:} \rightarrow
[\vdots] \rightarrow [\vdots]
\rightarrow
[:], 
\text{output: predictions}
$$
The above architecture assumes two-class classification (the output has two dots).
- *Optimizer*: The architecture of an ANN consists of input layer (which is the explanatory variables), hidden layer (if any), the output layer (tailored to the type of problems, regressor or classifier), and a loss function. Once the architecture is setup we can use an optimizer to find the weights that are used in the optimizer. A famous optimizer is called [gradient descent](https://towardsdatascience.com/gradient-descent-explained-9b953fc0d2c). The key of the gradient descent algorithm focuses on using iterated loops to update the parameters $w$ of the model a step at a time. At each step $s$, we compute the gradient of the loss function, i.e. $\nabla \mathcal{L}(\hat{Y}, Y)$. Next, we update the parameters: $w_s = w_{s+1} - \eta \cdot \nabla \mathcal{L}(\hat{Y}, Y)$. Here are some videos I posted: [Gradient Descent](https://www.youtube.com/watch?v=OtLSnzjT5ns), [Adam](https://www.youtube.com/watch?v=AqzK8LeRThM), [ANN Regressor Classifier Summary](https://www.youtube.com/watch?v=zhBLiMdqOdQ), related python scripts are posted [here](https://www.github.com/yiqiao-yin/YinsPy) 
- *Loss*: A loss function measures the distance between the predictions and the ground truth. The bigger the loss, the more mistakes there are in the model, and the worse the performance will be. The loss function is a choice given by the scientist or the user of the software package. A common loss function is *Mean Square Error*. Suppose we have ground truth $Y$ and the prediction $\hat{Y}$. The loss function *Mean Square Error* or *MSE* is defined as $\frac{1}{n} \sum_{i=1}^n (\hat{y_i} - y_i)^2$ while the running index $i$ indicates the observation. Another famous loss function is *Cross-entropy*. In binary classification problems, we recall the mathematical model of a Bernoulli random variable (i.e. recall the coin toss example). A random variable $X$ is said to have Bernoulli distribution or is a Bernoulli random variable if $\mathbb{P}(X=0) = p$ and $\mathbb{P}(X=1) = 1-p$ while $X = 0$ or $X = 1$. We can write $\mathbb{P}(X) = p^x (1-p)^{1-x}$ to be the mathematical model describing a coin-toss-like profile. Last, we take logarithm on the last step and obtain $x \log(p) + (1-p) \log(1-x)$. In reality, this $p$ is a predicted probability, so we replace it with $\hat{y}$. The $x$ is replaced with $y$ because it is the ground truth. Hence, we arrive with $y\log(\hat{y}) + (1 - y)\log(1 - \hat{y})$. A common list of lost function can be seen [here](https://towardsdatascience.com/most-common-loss-functions-in-machine-learning-c7212a99dae0).

## Convolutional Neural Network

Some additional sources:
- Computer Vision Feature Extraction: [post](https://towardsdatascience.com/computer-vision-feature-extraction-101-on-medical-images-part-1-edge-detection-sharpening-42ab8ef0a7cd)

## Recurrent Neural Network

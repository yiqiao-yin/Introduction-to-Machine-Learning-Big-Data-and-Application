# Representation Learning

There are three major types of *Representation Learning*.

## Artificial Neural Network

The architecture of a basic Artificial Neural Network (ANN) is the following
$$
\text{input variables:} \rightarrow
[\vdots] \rightarrow [\vdots]
\rightarrow
\text{output: predictions}
$$

When the notion of neural network is mentioned, we assume the architecture above with any particular sets of parameters (number of layers, number of neurons per layer, ..., i.e. these are all tuning parameters). ANN uses two concepts: *forward propagation* and *backward propagation*. 

- *Forward Propagation*: This operation allows information to pass from the input layer all the way to the output layer. There are linear components as well as non-linear components. Recall the basic linear regression and logistic regression models. We have explanatory variables $X$ and we first construct a linear combination of $\sum_{j=1}^p w_j X_j$ while the running index $j$ indicates the $j$th variable. This is then fed into the activation function (the famous ones are ReLU and sigmoid) to create predictions $\hat{Y}$. 
- *Backward Propagation*: This operation allows us to compare the predictions made by the educated guesses generated from the model with the ground truth, i.e. we compare how far away predictions $\hat{Y}$ is from the truth $Y$. This allows us to figure out the exact loss of a model. This is considered as an optimization problem with the following objective function:
$$\min_w \mathcal{L}(\hat{Y}, Y)$$
and the loss function is a matter of choice by the scientist or user of the algorithm. A list of loss functions can be found [here](https://towardsdatascience.com/most-common-loss-functions-in-machine-learning-c7212a99dae0). This above objective function states the following. The goal is to minimize the loss generated by prediction $\hat{Y}$ and the ground truth $Y$ while subject to the constraint of parameters (or weights) of the model $w$. In other words, we are allowed to change our weights $w$ until we found ourselves satisfied with the error generated with $\hat{Y}$ and $Y$. The quantity of error margin is a matter of choice and it usually is carried out as a relative term from model to model (i.e. this means we need a benchmark to compare and we do not just rely on one model).
- *Regressor*: This type of ANN learns from $X$ and produces an estimated value of $Y$ while $Y$ is continuous. The common loss function is [MSE](https://towardsdatascience.com/https-medium-com-chayankathuria-regression-why-mean-square-error-a8cad2a1c96f). In a neural network architecture that is designed as a regressor (predict a continuous variable, i.e. like regression model), we output one neuron.
$$
\text{input variables:} \rightarrow
[\vdots] \rightarrow [\vdots]
\rightarrow
[.], 
\text{output: predictions}
$$

- *Classifier*: This type of ANN learns from $X$ and produes an estimated probability of $Y$ that is a particular discrete value (aka factor or class). The common loss function is [binary cross-entropy](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a). For a neural network architecture that is designed as a classifier (predict a discrete variable, a class, or a label, i.e. like logistic model), we output a certain number of neurons (number should match the number of levels in the response variable). 
$$
\text{input variables:} \rightarrow
[\vdots] \rightarrow [\vdots]
\rightarrow
[:], 
\text{output: predictions}
$$
The above architecture assumes two-class classification (the output has two dots).
- *Optimizer*: The architecture of an ANN consists of input layer (which is the explanatory variables), hidden layer (if any), the output layer (tailored to the type of problems, regressor or classifier), and a loss function. Once the architecture is setup we can use an optimizer to find the weights that are used in the optimizer. A famous optimizer is called [gradient descent](https://towardsdatascience.com/gradient-descent-explained-9b953fc0d2c). The key of the gradient descent algorithm focuses on using iterated loops to update the parameters $w$ of the model a step at a time. At each step $s$, we compute the gradient of the loss function, i.e. $\nabla \mathcal{L}(\hat{Y}, Y)$. Next, we update the parameters: $w_s = w_{s+1} - \eta \cdot \nabla \mathcal{L}(\hat{Y}, Y)$. Here are some videos I posted: [Gradient Descent](https://www.youtube.com/watch?v=OtLSnzjT5ns), [Adam](https://www.youtube.com/watch?v=AqzK8LeRThM), [ANN Regressor Classifier Summary](https://www.youtube.com/watch?v=zhBLiMdqOdQ), related python scripts are posted [here](https://www.github.com/yiqiao-yin/YinsPy) 
- *Loss*: A loss function measures the distance between the predictions and the ground truth. The bigger the loss, the more mistakes there are in the model, and the worse the performance will be. The loss function is a choice given by the scientist or the user of the software package. A common loss function is *Mean Square Error*. Suppose we have ground truth $Y$ and the prediction $\hat{Y}$. The loss function *Mean Square Error* or *MSE* is defined as $\frac{1}{n} \sum_{i=1}^n (\hat{y_i} - y_i)^2$ while the running index $i$ indicates the observation. Another famous loss function is *Cross-entropy*. In binary classification problems, we recall the mathematical model of a Bernoulli random variable (i.e. recall the coin toss example). A random variable $X$ is said to have Bernoulli distribution or is a Bernoulli random variable if $\mathbb{P}(X=0) = 1-p$ and $\mathbb{P}(X=1) = p$ while $X = 0$ or $X = 1$. We can write $\mathbb{P}(X) = p^x (1-p)^{1-x}$ to be the mathematical model describing a coin-toss-like profile. Last, we take logarithm on the last step and obtain $x \log(p) + (1-p) \log(1-x)$. In reality, this $p$ is a predicted probability, so we replace it with $\hat{y}$. The $x$ is replaced with $y$ because it is the ground truth. Hence, we arrive with $y\log(\hat{y}) + (1 - y)\log(1 - \hat{y})$. A common list of lost function can be seen [here](https://towardsdatascience.com/most-common-loss-functions-in-machine-learning-c7212a99dae0).
- Sample code can be found in the following:
```
model <- keras_model_sequential() # sequential here means I am coding the structure layer by layer (it is not referring to sequential model)
# what is the model above?
# it is telling computer I want a keras sequential object
# this sequential object allows me to add dense layer or other NN layers
model %>%
  layer_dense(
    units = number.of.levels, # how many neurons
    input_shape = c(ncol(x_train)), # input shapes, this is the number of the columns
    activation = finalactivation, # "sigmoid", "tanh", "relu", .......
    use_bias = useBias # TRUE / FALSE (it is referring to beta_0 in the markdown file, i.e. recall beta_0 in linear model as well as logistic model)
  ) # adding one dense layer (or one layer)
summary(model)
```

## Convolutional Neural Network

Convolutional Neural Network (CNN) is built upon the understanding of a basic neural network. In addition, we make the assumption that we are accepting image data. This assumption implies two key information: (i) local information and its intrinsic value, (ii) geospatial architecture may have important relationship amonst each other. Based on this information and understanding of this type of data sets, we have a strong motivation of using a new tool to develop new features to feed in the neural network architecture.

The convolutional operation relies on combining matrices. Some basic matrix algebra can be found [here](https://towardsdatascience.com/basics-of-linear-algebra-for-data-science-9e93ada24e5c). The convolutional operation can be found [here](https://www.freecodecamp.org/news/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050/). The following digram is adopted from the sources above.

The convolutional operation can be illustrated in the following diagram. We have a filter (artificially designed) that intends to capture certain pattern in the picture. We apply this filter starting from the top left corner of the image. Then we roll this filter towards right first until we hit the last column. Once we are finished with one column we move the filter down one row and start from the left of the picture. We keep rolling this filter until we hit the bottom right corner of the image.
<p align="center"><img src="https://github.com/yiqiao-yin/Introduction-to-Machine-Learning-Big-Data-and-Application/blob/main/pics/basic-conv-op.png"></img></p>

The architecture below illustrates a simple Convolutional Neural Network (CNN) architecture and the basic forms of operation.
<p align="center"><img src="https://github.com/yiqiao-yin/Introduction-to-Machine-Learning-Big-Data-and-Application/blob/main/pics/basic-cnn.png"></img></p>

Sample code can be found in the following:
```
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = convl1, # number of filters
                kernel_size = convl1kernel, # the size of the filters
                activation = activation, # same with neural network => non-linear component => for general situation in case the assumptions of linearity break
                input_shape = input_shape # same with neural network => the only difference is instead of number of variables, we define number of rows and columns in the picture (matrix)
                ) %>%

  layer_conv_2d(filters = convl2, kernel_size = convl2kernel, activation = activation) %>%
  # layer_max_pooling_2d(pool_size = maxpooll1) %>% # maxpooling => given a 2x2 matrix, I 'm taking the maximum value => shrink dimension from 2x2=4 to 1 number 
  # layer_dropout(rate = 0.25) %>% # the percentage of data I drop every round of training
  layer_flatten() %>% # this function is a necessity and it reshapes the output matrices from the previous layer into a big long vector so that we can process neural network
  layer_dense(units = l1.units, activation = activation) %>% layer_dropout(rate = 0.5) %>%
  layer_dense(units = l2.units, activation = activation) %>% layer_dropout(rate = 0.5) %>%
  layer_dense(units = l3.units, activation = activation) %>% layer_dropout(rate = 0.5) %>%
  layer_dense(units = num_classes, activation = activationfinal)
```

Some additional sources:
- Computer Vision Feature Extraction: [post](https://towardsdatascience.com/computer-vision-feature-extraction-101-on-medical-images-part-1-edge-detection-sharpening-42ab8ef0a7cd)
- Building advanced model using Variational Auto-Encoder (VAE): [python](https://blog.keras.io/building-autoencoders-in-keras.html), [R](https://keras.rstudio.com/articles/examples/variational_autoencoder.html)
- Building advanced model using Generalized Adversarial Network (GAN): [python](https://keras.io/examples/generative/dcgan_overriding_train_step/), [R](https://blogs.rstudio.com/ai/posts/2018-08-26-eager-dcgan/)
- Avatarify: [github](https://github.com/alievk/avatarify-python). Please keep the application within a range of practice that does not harm the society of other people!
- Keras Examples: [python](https://keras.io/examples/), [R](https://tensorflow.rstudio.com/tutorials/)
- General AI Blog: [R AI Blog](https://blogs.rstudio.com/ai/)

## Recurrent Neural Network
